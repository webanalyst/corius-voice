# Copilot Instructions for Corius Voice

## Project Overview

Corius Voice is a native macOS app (Swift/SwiftUI) for real-time voice-to-text transcription, speaker diarization, and voice profile training. Inspired by Wispr Flow.

## Build & Development

### Quick Start

- Open `CoriusVoice.xcodeproj` in Xcode
- Build & Run the CoriusVoice target (âŒ˜R)

### Requirements

- macOS 14+
- Xcode 15+
- Microphone & Accessibility permissions

## Architecture

### App Structure

```
CoriusVoice/
â”œâ”€â”€ CoriusVoiceApp.swift        # App entry point & AppState
â”œâ”€â”€ Models/                     # Data models (RecordingSession, Speaker, Settings, etc.)
â”œâ”€â”€ Services/                   # Core services
â”œâ”€â”€ Views/                      # SwiftUI views
â”œâ”€â”€ ViewModels/                 # View models
â””â”€â”€ Utilities/                  # Helpers & extensions
```

### Key Services

- **RecordingService**: Recording flow, session lifecycle
- **DeepgramService**: Cloud transcription via WebSocket
- **WhisperService**: Local transcription with WhisperKit
- **LocalDiarizationService**: On-device diarization + 256-dim speaker embeddings (FluidAudio)
- **VoiceProfileService**: Voice training & speaker identification (embeddings preferred, legacy features as fallback)
- **StorageService**: Persistence (sessions, settings, notes)

## Recording & Training Flow

1. Record a session (mic/system/both)
2. Transcribe (Deepgram cloud or Whisper local)
3. Diarization identifies speakers (if enabled)
4. Assign speakers to library (link to known speakers)
5. Train voice profiles (uses 256-dim embeddings when available)
6. Auto-identify speakers in future sessions

## Voice Identification

Two methods available:

1. **Modern (Embeddings)**: 256-dim speaker embeddings from FluidAudio/WeSpeaker. Higher accuracy, preferred method.
2. **Legacy (Features)**: MFCCs, pitch, energy. Fallback when embeddings unavailable.

Training prioritizes embeddings â†’ extraction via FluidAudio â†’ legacy features fallback.

## Conventions

- Use `StorageService` for persistence
- Use `SpeakerLibrary` for known speakers
- Use `VoiceProfileService` for training/identification
- Keep UI changes in SwiftUI; preserve existing visual style
- Console logs use emoji prefixes (ğŸ¤, ğŸ§¬, ğŸ“, ğŸ”Š) for filtering

## Build & Verify

After every change:
1. Build in Xcode (âŒ˜B)
2. Check Issue Navigator for errors (âŒ˜â‡§M)
3. Fix any errors before proceeding

## Debugging

- Use Console.app to view logs
- Filter by process name or emoji prefixes
- Check System Settings for microphone/accessibility permissions
